# -*- coding: utf-8 -*-
"""B20AI052.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZbM8qZ1aTTHbw4Bj2yeO5TOVB8zj_2qj

Question 1

Preprocess the data and split in training and test set in 70:30 ratio and make sure all
classes are present in test data in approximately the same number. Visualize the training
data with scatterplot, using all possible combinations of two attributes.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import linalg
import math
from sklearn.metrics import confusion_matrix
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

df = pd.read_csv("iris_data.csv")
df

df.describe()

df.info()

values, counts = np.unique(df.iloc[:,-1], return_counts=True)
print(values)
print(counts)

from sklearn.model_selection import train_test_split

X, y = df.iloc[:,:-1], df.iloc[:,-1]
train_data, test_data = train_test_split(df, test_size=0.3, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

def plotting_scatterplots(df):
    for i in range(len(df.columns.drop('class'))):
        for j in range(i+1, len(df.columns.drop('class'))):
            if i != j:
                plt.figure(figsize=(5,5))
                sns.scatterplot(x=df.columns[i], y=df.columns[j], data=df, hue='class')
                plt.show()

plotting_scatterplots(df)

print(X_train)

"""Choose any three pairs of features (which you think will give good results), Train QDA
models for each pair, for the classification task. You can use the
QuadraticDiscriminantAnalysis function from sklearn.

Report the mean and covariance of the distributions found from each QDA model.
"""

pairs= [['sepal length','petal width'], ['petal length','petal width'], ['sepal width','petal width']]

X_train3 = X_train[pairs[0]].copy()
X_test3 = X_test[pairs[0]].copy()

X_train5 = X_train[pairs[1]].copy()
X_test5 = X_test[pairs[1]].copy()

X_train6 = X_train[pairs[2]].copy()
X_test6 = X_test[pairs[2]].copy()

qdaModel = QuadraticDiscriminantAnalysis(store_covariance=True)

qdaModel.fit(X_train3, y_train)
print("For the pair",pairs[0],"the mean is: ", qdaModel.means_)
print("For the pair",pairs[0],"the covariance is: ", qdaModel.covariance_)


qdaModel.fit(X_train5, y_train)
print("For the pair",pairs[1],"the mean is: ", qdaModel.means_)
print("For the pair",pairs[1],"the covariance is: ", qdaModel.covariance_)

qdaModel.fit(X_train6, y_train)
print("For the pair",pairs[2],"the mean is: ", qdaModel.means_)
print("For the pair",pairs[2],"the covariance is: ", qdaModel.covariance_)

"""Plot the decision boundary given by the QDA models on top of the corresponding scatterplot visualization of the data."""

plt.figure(figsize=(15,10))
plt.subplot(2,2,1)

sns.scatterplot(data=train_data, x='sepal length', y='petal width', hue="class", style='class')
qdaModel=QuadraticDiscriminantAnalysis()
qdaModel.fit(X_train3, y_train)
pred3 = qdaModel.predict(X_test3)

x_ax, y_ax = np.meshgrid(np.linspace(4, 8, 200), np.linspace(0, 2.5, 200))
X_grid = np.c_[x_ax.ravel(), y_ax.ravel()]
prob = qdaModel.predict_proba(X_grid)[:,1].reshape(x_ax.shape)
plt.contour(x_ax, y_ax, prob, [0.5])
plt.xlabel("Sepal Length")
plt.ylabel("Petal Width")
plt.title("QDA Sepal Length & Petal Width", fontsize=16)

plt.subplot(2,2,2)
sns.scatterplot(data=train_data, x='petal length', y='petal width', hue="class", style='class')

qdaModel.fit(X_train5, y_train)
pred5 = qdaModel.predict(X_test5)

x_ax, y_ax = np.meshgrid(np.linspace(0.5, 7, 200), np.linspace(0, 2.5, 200))
X_grid = np.c_[x_ax.ravel(), y_ax.ravel()]
prob = qdaModel.predict_proba(X_grid)[:,1].reshape(x_ax.shape)
plt.contour(x_ax, y_ax, prob, [0.5])
plt.xlabel("Petal Length")
plt.ylabel("Petal Width")
plt.title("QDA Petal Length & Petal Width", fontsize=16)

plt.subplot(2,2,3)
sns.scatterplot(data=train_data, x='sepal width', y='petal width', hue="class", style='class')

qdaModel.fit(X_train6, y_train)
pred6 = qdaModel.predict(X_test6)

x_ax, y_ax = np.meshgrid(np.linspace(1.5,4.5, 200), np.linspace(0, 2.5, 200))
X_grid = np.c_[x_ax.ravel(), y_ax.ravel()]
prob = qdaModel.predict_proba(X_grid)[:,1].reshape(x_ax.shape)
plt.contour(x_ax, y_ax, prob, [0.5])
plt.xlabel("Sepal Width")
plt.ylabel("Petal Width")
plt.title("QDA Sepal Width & Petal Width", fontsize=16)
plt.xlim(1.5,5)

plt.show()

"""Predict the test data and report error rate for each case."""

def error_rate(pred, actual):
    return np.mean(pred != actual)

print("Error rate when qda model is trained with sepal length and petal width features :",error_rate(pred3, y_test))
print("Error rate when qda model is trained with petal length and petal width features :",error_rate(pred5, y_test))
print("Error rate when qda model is trained with sepal width and petal width features :",error_rate(pred6, y_test))

print("Either of ",pairs[0],"or",pairs[1],"are giving the best results because of the least error rates.")

"""Take the pair of features that has given the best result and train LDA model with same training data. You can use the LinearDiscriminantAnalysis function from sklearn.

Plot the decision boundary given by the LDA model on top of the scatterplot visualization of the data.
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
ldaModel = LinearDiscriminantAnalysis(store_covariance=True)

ldaModel.fit(X_train5, y_train)
pred_lda = ldaModel.predict(X_test5)

plt.figure(figsize=(15,10))
sns.scatterplot(data=train_data, x='petal length', y='petal width', hue="class", style='class')

x_ax, y_ax = np.meshgrid(np.linspace(0.9,7, 200), np.linspace(0, 2.5, 200))
X_grid = np.c_[x_ax.ravel(), y_ax.ravel()]
prob = ldaModel.predict_proba(X_grid)[:,1].reshape(x_ax.shape)
plt.contour(x_ax, y_ax, prob, [0.5])
plt.xlabel("Petal Length")
plt.ylabel("Petal Width")
plt.title("LDA Petal Length & Petal Width", fontsize=16)
plt.show()

"""Report the error rate on the test data for LDA model."""

print("Error rate when LDA model is trained with", pairs[1],"as features :",error_rate(pred_lda, y_test))

import matplotlib as mpl    

def plot_ellipse(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1]/u[0])
    angle = 180 * angle / np.pi # convert to degrees
    ell = mpl.patches.Ellipse(mean, 4 * v[0] ** 0.5, 4 * v[1] ** 0.5,
                                            180 + angle, color=color)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.5)
    splot.add_artist(ell)
    splot.plot(mean[0],mean[1],"*",color="yellow",markersize=15)

"""Visualize the gaussian distributions obtained from QDA and LDA."""

qda_new = QuadraticDiscriminantAnalysis(store_covariance=True)
lda_new = LinearDiscriminantAnalysis(store_covariance=True)
qda_new.fit(X_train5, y_train)
lda_new.fit(X_train5, y_train)

plt.figure(figsize=(15,10))
newplt = plt.subplot(2,2,1)
sns.scatterplot(data=train_data, x='petal length', y='petal width', hue="class", style='class')
plt.xlabel("Petal Length")
plt.ylabel("Petal Width")
plt.title("QDA Analysis for Petal Length & Petal Width", fontsize=16)
plot_ellipse(newplt, qda_new.means_[0], qda_new.covariance_[0], 'red')
plot_ellipse(newplt, qda_new.means_[1], qda_new.covariance_[1], 'green')
plot_ellipse(newplt, qda_new.means_[2], qda_new.covariance_[2], 'blue')

newplt2 = plt.subplot(2,2,2)
sns.scatterplot(data=train_data, x='petal length', y='petal width', hue="class", style='class')
plt.xlabel("Petal Length")
plt.ylabel("Petal Width")
plt.title("LDA Analysis for Petal Length & Petal Width", fontsize=16)
plot_ellipse(newplt2, lda_new.means_[0], lda_new.covariance_, 'red')
plot_ellipse(newplt2, lda_new.means_[1], lda_new.covariance_, 'green')
plot_ellipse(newplt2, lda_new.means_[2], lda_new.covariance_, 'blue')

plt.show()

"""Question 2

From the data, find out the sample mean and sample covariance matrix of each class and visualize the gaussian distribution using the obtained sample mean and sample covariance matrix.
"""

df

df2 = df[['petal length', 'petal width','class']]
df2

values = np.unique(df2['class'])
print(values)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df2['class'] = encoder.fit_transform(df2['class'])
df2

def get_mean(data):
    return sum(data)/len(data)

def covariance(data,cls):
    matrix = []
    mean_x = get_mean(data[data['class']==cls]['petal length'])
    mean_y = get_mean(data[data['class']==cls]['petal width'])
    cov_xy1 = sum((data[data['class']==cls]['petal length']-mean_x)**2)/len(data[data['class']==cls]['petal length'])
    cov_xy2 = sum((data[data['class']==cls]['petal length']-mean_x)*(data[data['class']==cls]['petal width']-mean_y))/len(data[data['class']==cls]['petal length'])
    cov_xy3 = sum((abs(data[data['class']==cls]['petal width']-mean_x))*(data[data['class']==cls]['petal length']-mean_y))/len(data[data['class']==cls]['petal length'])
    cov_xy4 = sum((data[data['class']==cls]['petal width']-mean_y)**2)/len(data[data['class']==cls]['petal width'])
    matrix = [[cov_xy1, cov_xy2],[cov_xy3, cov_xy4]]
    return matrix

cls1 = df2[df2['class'] == 0]
cls2 = df2[df2['class'] == 1]
cls3 = df2[df2['class'] == 2]

print("Mean of class Iris-setosa :",[get_mean(cls1['petal length']),get_mean(cls1['petal width'])])
print("Covariance of class Iris-setosa :",covariance(cls1,0))

print("Mean of class Iris-versicolor :",[get_mean(cls2['petal length']),get_mean(cls2['petal width'])])
print("Covariance of class Iris-versicolor :",covariance(cls2,1))

print("Mean of class Iris-virginica :",[get_mean(cls3['petal length']),get_mean(cls3['petal width'])])
print("Covariance of class Iris-virginica :",covariance(cls3,2))

def plot_ellipse_2(splot, mean, cov, color):
    v, w = linalg.eigh(cov)
    u = w[0] / linalg.norm(w[0])
    angle = np.arctan(u[1]/u[0])
    angle = 180 * angle / np.pi # convert to degrees
    ell = mpl.patches.Ellipse(mean, 4 * v[0] ** 0.5, 4 * v[1] ** 0.5,
                                            180 + angle, color=color)
    ell.set_clip_box(splot.bbox)
    ell.set_alpha(0.5)
    splot.add_artist(ell)
    splot.plot(mean[0],mean[1],"*",color="yellow",markersize=15)

plt.figure(figsize=(15,10))
newplt = plt.subplot(2,2,1)
sns.scatterplot(data=df2, x='petal length', y='petal width', hue="class", style='class')

mean1 = [get_mean(cls1['petal length']),get_mean(cls1['petal width'])]
cov1 = covariance(cls1,0)
#plot_ellipse_2(newplt, mean1, cov1, 'red')

mean2 = [get_mean(cls2['petal length']),get_mean(cls2['petal width'])]
cov2 = covariance(cls2,1)
#plot_ellipse_2(newplt, mean2, cov2, 'green')

mean3 = [get_mean(cls3['petal length']),get_mean(cls3['petal width'])]
cov3 = covariance(cls3,2)
#plot_ellipse_2(newplt, mean3, cov3, 'blue')

plt.show()

from sklearn.model_selection import train_test_split
df2

X = df2
y = df2['class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""Write a function compute_likelihood to compute the likelihood of data given the parameters mean and covariance matrix assuming gaussian distribution."""

def compute_likelihood(df, parameters, target):
    new_df = df.copy()
    new_df.drop(target, axis=1, inplace=True)
    probability = 1
    for i in range(len(df)):
        row = new_df[i:i+1]
        x = np.cross((row - parameters[0]), np.linalg.inv(parameters[1]))
        y = np.array(np.transpose(row-parameters[0]))
        cr_product = x[0]*y[0][0] + x[1]*y[1][0]
        probability *= np.exp(-1/2*cr_product)/np.sqrt(2*np.pi*np.linalg.det(parameters[1]))

    return probability

def get_probab(dataset,cov_matrix,mean,cls):
  Matrix=[]
  for rows in dataset.iterrows():
    x=np.array([rows[1][0],rows[1][1]])
    comp_1=np.subtract(x,mean)
    trans_comp_1=comp_1.transpose()
    COV=np.linalg.inv(cov_matrix)
    result=comp_1 @ COV @ trans_comp_1
    det=(np.linalg.det(cov_matrix))**0.5
    final=(1/3)*(1/(2*3.14*det))*np.exp(((-1/2)*result))
    Matrix.append(final)
  return Matrix

"""Write a function to perform maximum likelihood estimation over the training dataset to determine mean and covariance and classify it using Bayes classifier. Report the parameters obtained for each class."""

def max_likelihood_est(df, feature, target):
    unq_class = list(df[target].unique())
    unq_class.sort()
    means = []
    cov = []
    for i in unq_class:
        x = df.loc[df[target]==i]
        n = len(x)
        means.append([get_mean(list(x[feature[0]])), get_mean(list(x[feature[1]]))])
        co1 = covariance(list(x[feature[0]]), list(x[feature[0]]))*(n-1)/n
        co2 = covariance(list(x[feature[1]]), list(x[feature[0]]))*(n-1)/n
        co3 = covariance(list(x[feature[1]]), list(x[feature[1]]))*(n-1)/n
        cov.append([[co1, co2],[co2,co3]])

    
    return means, cov

def Prior_Prob(y, value):          
    return len(y[y==value])/len(y)

def get_MLE(dataset,cls):
  mean=[mean(dataset,'petal length',cls),mean(dataset,'petal width',cls)]
  variance=np.var(dataset[['petal length','petal width']].values,axis=0)
  var=np.multiply(variance,[[1,0],[0,1]])
  return (mean,var)