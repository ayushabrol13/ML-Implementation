# -*- coding: utf-8 -*-
"""PRML_Lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nESCUqtzBB7yhUy_rfI7T5QefYSYWL09
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Question 1"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from random import randrange
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, make_scorer, f1_score, r2_score

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Housing.csv')

"""## Visualizing the data"""

sns.pairplot(df, hue = 'price')

df.info()

"""## Preprocessing (Dropping Nulls, Encoding object features)"""

df = df.dropna()

df.head()

cat_feature = list(df.select_dtypes(include = ['object']).columns)
cat_feature

encoder = LabelEncoder()
for col in cat_feature:
    df[col] = encoder.fit_transform(df[col])
df.head()

"""## Splitting the data into Training and Test Sets"""

X = df.drop(['price'], axis=1)
y = df['price'] 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

X_train

y_train

"""## Using a simple Decision Tree regressor to predict the price of a house (without any validation) and finding the accuracy."""

dtreregressor = DecisionTreeRegressor(max_depth=3)
dtreregressor.fit(X_train, y_train)
y_pred = dtreregressor.predict(X_test)

acc = r2_score(y_test, y_pred)
print("Accuracy:", acc)

"""## Performing 5-fold cross-validation to determine what the best max_depth would be for a single regression tree using the entire 'Xtrain' feature set and visualizing the results."""

def best_max_depth_using_cross_validation_score(X, y):
    max_depth_range = range(1, 31)
    scores = []
    for max_depth in max_depth_range:
        dtreregressor = DecisionTreeRegressor(max_depth=max_depth)
        scores.append(cross_val_score(dtreregressor, X, y, scoring='r2', cv=5).mean())
    plt.plot(max_depth_range, scores)
    plt.title("Max Depth vs R2_Scores")
    plt.xlabel('Max_depth')
    plt.ylabel('R2 score')
    plt.show()  
    return max_depth_range[scores.index(max(scores))]

ans = best_max_depth_using_cross_validation_score(X_train, y_train)
print("\nBest max_depth:", ans)

"""## Applying bagging to create different training datasets and training on different dataset to obtain different decision trees."""

def bagging(X, y, max_depth_b, n_estimators=10):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
    regressor_list = []
    pred_list = []
    r2_scores = []
    for i in range(n_estimators):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)
        dtreregressor = DecisionTreeRegressor(max_depth = max_depth_b)
        dtreregressor.fit(X_train, y_train)
        y_pred = dtreregressor.predict(X_test)
        regressor_list.append(dtreregressor)
        pred_list.append(y_pred)
    
    return regressor_list, pred_list, r2_scores, y_test

regressor_list, pred_list, r2_scores, new_y_test = bagging(X_train, y_train, ans, n_estimators=10)

"""## Summarizing how each of the separate trees performed (both numerically and visually) using R-squared score as the metric.

### Numerical
"""

for i in range(len(pred_list)):
        score = r2_score(new_y_test, pred_list[i])
        r2_scores.append(score)
        print(f"R2 Score for Tree Number {i+1} is",score)

"""### Visual"""

plt.plot([int(x) for x in range(1,11,1)], r2_scores)
plt.title("Visualization of Bagging Trees")
plt.xlabel('Tree Number')
plt.ylabel('Predicted Price')
plt.show()

"""## How they performed on average."""

print("\nAverage R2 Score of all trees:", np.mean(r2_scores))

"""# Combining the trees into one prediction and evaluating it using R-squared score."""

def combining_bagging_trees(regressor_list, pred_list, r2_scores):
    final_pred_list = []
    for i in range(len(pred_list)):
        final_pred_list.append(pred_list[i])
    final_pred_list = np.array(final_pred_list)
    final_pred_list = np.mean(final_pred_list, axis=0)
    final_pred_list = final_pred_list.reshape(-1,1)
    return final_pred_list

final_y_pred = combining_bagging_trees(regressor_list, pred_list, r2_scores)
print(final_y_pred)

print("\nFinal R2 Score of Combined Trees:", r2_score(new_y_test, final_y_pred))

"""## When Max Depth is increased."""

new_increased_depth = best_max_depth_using_cross_validation_score(X_train, y_train) + 2
print("\nNew Max Depth:", new_increased_depth)

inc_regressor_list, inc_pred_list, inc_r2_scores, inc_new_y_test = bagging(X_train, y_train, new_increased_depth, n_estimators=10)
for i in range(len(inc_pred_list)):
    score = r2_score(inc_new_y_test, inc_pred_list[i])
    inc_r2_scores.append(score)
    print(f"R2 Score for Tree Number {i+1} is",score)

plt.plot([int(x) for x in range(1,11,1)], inc_r2_scores)
plt.title("Visualization of Bagging Trees for Increased Max Depth")
plt.xlabel('Tree Number')
plt.ylabel('Predicted Price')
plt.show()

inc_final_y_pred = combining_bagging_trees(inc_regressor_list, inc_pred_list, inc_r2_scores)
print("\nFinal R2 Score of Combined Trees for Increased Max Depth:", r2_score(inc_new_y_test, inc_final_y_pred))

"""## When Max Depth is decreased"""

new_decreased_depth = best_max_depth_using_cross_validation_score(X_train, y_train) - 1
print("\nNew Max Depth:", new_decreased_depth)

dec_regressor_list, dec_pred_list, dec_r2_scores, dec_new_y_test = bagging(X_train, y_train, new_decreased_depth, n_estimators=10)
for i in range(len(dec_pred_list)):
    score = r2_score(dec_new_y_test, dec_pred_list[i])
    dec_r2_scores.append(score)
    print(f"R2 Score for Tree Number {i+1} is",score)

plt.plot([int(x) for x in range(1,11,1)], dec_r2_scores)
plt.title("Visualization of Bagging Trees for Decreased Max Depth")
plt.xlabel('Tree Number')
plt.ylabel('Predicted Price')
plt.show()

dec_final_y_pred = combining_bagging_trees(dec_regressor_list, dec_pred_list, dec_r2_scores)
print("\nFinal R2 Score of Combined Trees for Decreased Max Depth:", r2_score(dec_new_y_test, dec_final_y_pred))

"""## Training random forest regressor, reporting mean squared error and mean absolute error."""

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

random_forest_regressor = RandomForestRegressor(n_estimators=50)
random_forest_regressor.fit(X_train, y_train)
y_pred = random_forest_regressor.predict(X_test)

print(y_test)

y_pred = pd.Series(y_pred)
print(y_pred)

print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("Mean Absolute Error:", mean_absolute_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))

"""## Training Adaboost regressor and reporting mean squared error and mean absolute error."""

ada_boost_regressor = AdaBoostRegressor(n_estimators = 50)
ada_boost_regressor.fit(X_train, y_train)
y_pred = ada_boost_regressor.predict(X_test)

print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("Mean Absolute Error:", mean_absolute_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))

"""# Question 2

## Installing xgboost and lightgbm libraries.
"""

!pip install xgboost
!pip install lightgbm

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier

df2 = pd.read_csv("/content/drive/My Drive/Colab Notebooks/Breast_cancer_data.csv")

"""## Visualizing the given data"""

sns.pairplot(df2, hue = 'diagnosis')

df2.info()

"""## Preprocessing the data (including dropping nulls)"""

df2 = df2.dropna()

"""## Splitting the dataset into training and testing sets"""

X2 = df2.drop(['diagnosis'], axis=1)
y2 = df2['diagnosis']
X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=0)

"""## Using a simple Decision Tree classifier to predict the outcome (without any validation) and reporting the accuracy"""

def DecisionTreeClassifier_acc(X_train, X_test, y_train, y_test):
    dtclassifier = DecisionTreeClassifier(random_state = 0)
    dtclassifier.fit(X_train, y_train)
    y_pred = dtclassifier.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    return acc

print("Accuracy of Decision Tree Classifier is:", DecisionTreeClassifier_acc(X_train2, X_test2, y_train2, y_test2))

"""## Performing 5-fold cross-validation to determine what the best max_depth would be for a single regression tree using the entire 'Xtrain' feature set."""

def best_max_depth_using_cross_validation_score(X, y):
    max_depth_range = range(1, 101)
    scores = []
    for max_depth in max_depth_range:
        dtreregressor = DecisionTreeRegressor(max_depth=max_depth)
        scores.append(cross_val_score(dtreregressor, X, y, scoring='r2', cv=5).mean())
    plt.plot(max_depth_range, scores)
    plt.title("Max Depth vs. Cross Validation Scores")
    plt.xlabel('Max_depth')
    plt.ylabel('R2 score')
    plt.show()  
    return max_depth_range[scores.index(max(scores))]

"""## Visualizing and summarizing the results"""

ans = best_max_depth_using_cross_validation_score(X_train2, y_train2)
print("Best max_depth:", ans)

"""## Implementing XGBoost in which subsample=0.7 and max_depth=4."""

import xgboost as xgb
def xgboost(X_train, X_test, y_train, y_test):
    xg_reg = xgb.XGBClassifier(objective="binary:logistic", subsample =0.7, max_depth=4, n_estimators=10)
    xg_reg.fit(X_train, y_train)
    y_pred = xg_reg.predict(X_test)
    # print(y_test)
    # print(y_pred)
    acc = accuracy_score(y_test, y_pred)
    return acc

xgboost_accuracy_train = xgboost(X_test2, X_train2, y_test2, y_train2)
xgboost_accuracy_test = xgboost(X_train2, X_test2, y_train2, y_test2)
print("The accuracy obtained on the training set is: ", xgboost_accuracy_train)
print("The accuracy obtained on the testing set is: ", xgboost_accuracy_test)

"""## Implementing LightGBM with max_depth value as 3 and choosing different values for num_leaves."""

import lightgbm as lgb
def lightBGM(X_train, X_test, y_train, y_test, max_depth_given, num_leaves_given):
    lgbm = lgb.LGBMClassifier(max_depth=max_depth_given, num_leaves=num_leaves_given)
    lgbm.fit(X_train, y_train)
    y_pred = lgbm.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    return acc

for i in range(2, 15):
    print("LightGBM with max_depth=3 and num_leaves=", i, ":", lightBGM(X_train2, X_test2, y_train2, y_test2, 3, i))

"""## Analysing the relation between max_depth and num_leaves, and checking for the value at which the model starts overfitting."""

val = 8
for i in range(2,15):
    print(f"LightGBM with max_depth = {i} and num_leaves = {val} : {lightBGM(X_train2, X_test2, y_train2, y_test2, i, val)}")

"""## Reporting which parameters can be used for better accuracy and also which parameter can be used for avoiding overfitting."""

for i in range(1,20,1):
    lgbm_new  = lgb.LGBMClassifier(max_depth=6, num_leaves=8, min_data_in_leaf = i)
    lgbm_new.fit(X_train2, y_train2)
    y_pred = lgbm_new.predict(X_test2)
    acc = accuracy_score(y_test2, y_pred)
    print("LightGBM with max_depth=6, num_leaves=8, min_data_in_leaf=", i, ":", acc)